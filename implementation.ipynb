{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad667fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-ollama ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2438fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251849eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277dfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alucinações de input\n",
    "def has_input_hallucination_llm(question, answer):\n",
    "    inputHallucinationTemplate = \"\"\"\n",
    "        Você é um assistente cuja função é responder se houve ou não alucinações de input.\n",
    "        Considere que de alucinação de input é quando a resposta foge do tópico da pergunta feita\n",
    "        pelo usuário.\n",
    "\n",
    "        Exemplo 1:\n",
    "            Pergunta: Quais as raças de gato de maior tamanho existentes?\n",
    "            Resposta: Claro! Aqui estão as maiores raças de cachorro\n",
    "        \n",
    "        Exemplo 2: \n",
    "            Pergunta: Me diga fatos sobre a cidade do Recife.\n",
    "            Resposta: Os recifes de coral são a maior estrutura viva do planeta.\n",
    "\n",
    "        Exemplo 3:\n",
    "            Pergunta: Preciso de sugestões para cortes em cabelos cacheados\n",
    "            Resposta: Cabelos lisos ficam ótimos com o corte borboleta.\n",
    "        \n",
    "        Considerando que a resposta gerada pergunta \"{question}\" foi \"{answer}\", houve alucinação de input?\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(inputHallucinationTemplate)\n",
    "    chain = prompt | model\n",
    "\n",
    "    result = chain.invoke({\"question\": question, \"answer\": answer })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238bbead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sim, houve alucinação de input!\\n\\nA pergunta original é \"Me de uma sugestão do que comer no jantar\", e a resposta \"Claro! Aqui estão sugestões de almoço\" não atende ao tópico da pergunta. A resposta está falhando em fornecer sugestões para o jantar, ao invés disso, oferece sugestões para o almoço.\\n\\nIsso é um exemplo de alucinação de input, pois a resposta não está relacionada à pergunta original.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Me de uma sugestão do que comer no jantar\"\n",
    "answer = \"Claro! Aqui estão sugestões de almoço\"\n",
    "\n",
    "has_input_hallucination_llm(question, answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
